# ğŸ§  LSS Report Generator

AI-based tool for generating diagnostic findings and analyses from spine-related imaging data using local large language models (LLMs) like Mistral or LLaMA via the Ollama framework. This project supports advanced reasoning strategies like Retrieval-Augmented Generation (RAG), including both agentic and fusion types.

---

## ğŸš€ Features

- âœ… **LLM-based Findings & Analysis Generation**
- ğŸ“š **Agentic and Fusion RAG Support**
- ğŸ” **Knowledge Graph or RAG Contextual Retrieval**
- ğŸ“¤ **Excel Report Export for Each Patient**
- âš™ï¸ **Modular and Testable Codebase**
- ğŸ§¾ **Command-Line Interface (CLI) Support**
- ğŸªµ **Logging to Console and File**

---

## ğŸ“ Directory Structure

```
project_root/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ JSON_data_for_testing_dicom_V3_deformities.json
â”œâ”€â”€ logs/
â”‚   â””â”€â”€ report_generation.log
â”œâ”€â”€ results/
â”‚   â””â”€â”€ [auto-generated results per LLM/RAG type]
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py
â”‚   â”œâ”€â”€ utils.py
â”‚   â””â”€â”€ prompts.py
â”œâ”€â”€ run.sh
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

---

## âš™ï¸ Setup

### 1. Clone the Repository

```bash
git clone
cd lss-report-generator
```

### 2. Create a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate  # For Linux/macOS
# venv\Scripts\activate   # For Windows
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

---

## ğŸ§ª How to Run

### Using Python CLI

```bash
python3 src/main.py --llm mistral --retrieval RAG --ragtype agentic
```

### Using Bash Script

```bash
chmod +x run.sh
./run.sh
```

---

## ğŸ›ï¸ CLI Parameters

| Argument      | Description                           | Default   |
|---------------|---------------------------------------|-----------|
| --llm         | Name of LLM model (e.g., mistral)     | mistral   |
| --retrieval   | Retrieval strategy: RAG or KG         | RAG       |
| --ragtype     | RAG subtype: agentic or fusion        | agentic   |

---

## ğŸ“¤ Output

- **Log File:** `logs/report_generation.log`
- **Excel Report:**  
  `results_RAG_agentic/mistral_patient_predictions.xlsx`  
  Each row contains:
  - Patient ID
  - Predicted findings
  - Final analysis
  - Time metrics for each step

---

## ğŸ§  Supported Models

You can run any local LLM supported by Ollama:

- Gemma3
- mistral
- llama3
- phi3
- Qwen3

## Supported Databases

You can run any Database of the following:

- Vector DB
- Knowledge Graph DB

## ğŸ› ï¸ Code Overview
**src/main.py**:
Main entry point. Handles CLI arguments and API requests, orchestrates report generation using the LSSReportGenerator class.

**src/utils.py**:
Utility functions for DICOM processing, retrieval (RAG/KG), logging, and database connections.

**src/prompts.py**:
Prompt templates for findings and analysis generation.

**Data & Results**:
Place DICOM files in the appropriate data folder.
Generated reports and logs are saved in **results/** and **logs/**.